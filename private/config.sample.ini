[s3.source]
access_key = access_key
secret_key = secret_key
endpoint_url = https://cos.ap-hongkong.myqcloud.com
bucket = source-00000

[s3.destination]
access_key = access_key
secret_key = secret_key
endpoint_url = https://cos.ap-hongkong.myqcloud.com
bucket = destination-00000

[pg.database]
; just use the remote localhost directly if using a tunnel
host = 127.0.0.1
port = 5432
user = postgres
password = password
database = postgres

[pg.tunnel]
; in most cases, we shouldn't expose the database to the public network,
; so we use a ssh tunnel to connect to the remote database.
; but you may not need this if you are running this program on the same
; remote server.
enabled = true
ssh_host = your_server_ip
ssh_port = 22
ssh_user = root
ssh_password =
; file path: PROJECT_ROOT/private/id_ed25519
ssh_key = id_ed25519
; the local host and port to forward the remote database to
local_host = 127.0.0.1
local_port = 5432

[celery]
broker = redis://localhost:6379/0
backend = redis://localhost:6379/0
log_level = DEBUG
; We use memory to cache the downloaded temporary files, and the memory
; will be release after one task is finished. With Mastodon's default
; media attachment size limit, up to 150Mb memory will be used for each
; task (concurrency).
; You can increase the swap space to run with a higher concurrency and
; avoid OOM.
concurrency = 5

